---
title: "Milestone Report"
author: "chen0350"
date: "7/21/2020"
output: html_document
---
## A. Introduction

The goal of this project is mainly to display that I have gotten used to working with the data and that I am on track to create your prediction algorithm.

I will explain only the major features of the data I have identified and briefly summarize my plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. 

## B. Basic Data Exploration

We load libraries used for this project.

We create two functions which will help us understand the basic information of our text files: the number of Lines and number of Words that compose the data.

We apply the functions to our datasets, namely English Twitter, Blog and News.

```{r echo=FALSE, results='hide'}
library(NLP)
library(tm)
library(RWeka)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(LaF)
library(knitr)
```

```{r echo=FALSE, results="hide"}
# Create a function which gets the number of lines
# In order to not load the full data, we won't be using readText, but readBin instead,
# using a remote connexion
get_line_count <- function(file_name) {
    
    con <- file(file_name, open="rb")
    len_file <- 0
    while (length(chunk <- readBin(con, "raw", 65536)) > 0) {
        len_file <- len_file + sum(chunk == as.raw(10L))
    }
    close(con)
    
    rm(con, chunk)
    
    return(len_file)
}


# Create a function which gets the number of word
# Likewise, we use a connexion and read Line by Line
get_word_count <- function(file_name, len_file) {
    
    con <- file(file_name, "r")
    len_word <- 0
    for (i in seq(1:len_file)) {
        sentence = readLines(con, 1)
        sentence = strsplit(sentence, " ")
        sentence = unlist(sentence, use.names=F)
        len_word = len_word + length(sentence)
    }
    close(con)
    rm(sentence, con, i)
    
    return(len_word)
}
```

```{r echo=FALSE}
file_name <- "en_US.twitter.txt"
tweet_length <- get_line_count(file_name)
tweet_words <- get_word_count(file_name, tweet_length)

file_name <- "en_US.blogs.txt"
blogs_length <- get_line_count(file_name)
blogs_words <- get_word_count(file_name, blogs_length)

file_name <- "en_US.news.txt"
news_length <- get_line_count(file_name)
news_words <- get_word_count(file_name, news_length)

base_info <- data.frame("Source"=c("Twitter", "Blogs", "News"), 
                        "Line Count"=c(tweet_length, blogs_length, news_length), 
                        "Words count"=c(tweet_words, blogs_words, news_words))

kable(format(base_info, big.mark = ","), caption = "Base Information", align=rep("c", 3))
```


## C. N-gram exploration

Process the text using the “tm” package, which transforms the dataset into a Corpus. 

I use a function which transforms the text file into a Corpus, process it (“tm” package) and transforms it into a Frequency dataframe (“RWeka” package).

I used a random sampling based on the LaF package, where I retain 0.3% of the original corpus. 

Once the random sampling done (using the “LaF” package “sample_lines” function), we will plot a frequency table for the first 20th most frequent words, up to a 4-Gram table.

```{r echo=FALSE, results="hide"}
# Create a function which process the data (given we will apply it to the 3 documents)
text_processing <- function(text, n) {
    
    # Determine the "n" to apply to the RWeka N-gram tokenizer
    # "n" has been ceiled to 3 but the same logic can be extended for bigger "n" if needed be
    if (n == 1) {
        n_gram = function(x) {NGramTokenizer(x, Weka_control(min=1, max=1))}
    } else if (n == 2) {
        n_gram = function(x) {NGramTokenizer(x, Weka_control(min=2, max=2))}
    } else {
        n_gram = function(x) {NGramTokenizer(x, Weka_control(min=3, max=3))}
    }
    
    # Define a string replacing function to help tm_map
    kill_chars <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
    
    df <- text %>% 
    # Converting the text into a Corpus thanks to the tm package
        VectorSource() %>%
        VCorpus() %>% 
        
    # Process the corpus with the "tm_map" function
    # Note that the basic functions are almost all used except for stemming
    # which shouldn't be used for this exercise
        tm_map(content_transformer(tolower)) %>%
        tm_map(removePunctuation) %>%
        tm_map(stripWhitespace) %>%
        tm_map(removeNumbers) %>%
        tm_map(removeWords, stopwords("english")) %>%

    # Remove all special characters (which can't be treated with TermDocumentMatrix)
        tm_map(kill_chars, "/") %>%
        tm_map(kill_chars, "@") %>%
        tm_map(kill_chars, "\\|") %>%      
        tm_map(kill_chars, "â") %>%
        tm_map(kill_chars, "<") %>%
        tm_map(kill_chars, "~") %>%
        tm_map(kill_chars, "#") %>%
        tm_map(kill_chars, "ÿ") %>%
        tm_map(kill_chars, "ð") %>%
        tm_map(kill_chars, "®") %>%
        tm_map(kill_chars, "€") %>%
        tm_map(kill_chars, "™") %>%
        tm_map(kill_chars, "¬") %>%
        tm_map(kill_chars, "•") %>%
        tm_map(kill_chars, "œ") %>%
        tm_map(kill_chars, "“") %>%
        tm_map(kill_chars, "¦") %>%
        tm_map(kill_chars, "ž") %>%
        tm_map(kill_chars, "¤") %>%
        tm_map(kill_chars, "¶") %>%
        
    # Transform the output into a dataframe (through a Matrix), with the word as first column
        TermDocumentMatrix(control = list(tokenize = n_gram)) %>%
        as.data.frame.matrix() %>%
        mutate(word = row.names(.)) %>%
        
    # Sum all all column and keep sum only, to have a Word/Frequency table
        mutate(frequency = rowSums(select(., -"word"))) %>%
        arrange(desc(`1`)) %>%
        select(word, frequency) %>%
        arrange(desc(frequency))
    
    return(df)
}
```

```{r echo=FALSE}
file_name <- "en_US.twitter.txt"
tweet_length_sample <- round(as.numeric(get_line_count(file_name)) * 0.003)
tweet_words_sample <- get_word_count(file_name, tweet_length_sample)

file_name <- "en_US.blogs.txt"
blogs_length_sample <- round(as.numeric(get_line_count(file_name)) * 0.003)
blogs_words_sample <- get_word_count(file_name, blogs_length_sample)

file_name <- "en_US.news.txt"
news_length_sample <- round(as.numeric(get_line_count(file_name)) * 0.003)
news_words_sample <- get_word_count(file_name, news_length_sample)

base_info <- data.frame("Source"=c("Twitter", "Blogs", "News"), 
                        "Line Count"=c(tweet_length_sample, blogs_length_sample, news_length_sample), 
                        "Words count"=c(tweet_words_sample, blogs_words_sample, news_words_sample))

kable(format(base_info, big.mark = ","), caption = "After sampling", align=rep("c", 3))
```




## C (i) Twitter
```{r echo=FALSE}
file_name <- "en_US.twitter.txt"
tweet_info <- text_processing(sample_lines(file_name, n=tweet_length*0.003, tweet_length), 1)
plotDf <- tweet_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g1 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity", fill="blue") + coord_flip() +
    ggtitle("Twitter: 1-Gram Frequency Plot")

tweet_info <- text_processing(sample_lines(file_name, n=tweet_length*0.003, tweet_length), 2)
plotDf <- tweet_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g2 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity", fill="blue") + coord_flip() +
    ggtitle("Twitter: 2-Gram Frequency Plot")

tweet_info <- text_processing(sample_lines(file_name, n=tweet_length*0.003, tweet_length), 3)
plotDf <- tweet_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g3 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity", fill="blue") + coord_flip() +
    ggtitle("Twitter: 3-Gram Frequency Plot")
```

```{r echo=FALSE}
g1
```

```{r echo=FALSE}
g2
```


```{r echo=FALSE}
g3
```

## C (ii) Blogs 

```{r echo=FALSE}
file_name <- "en_US.blogs.txt"
blogs_info <- text_processing(sample_lines(file_name, n=blogs_length*0.003, blogs_length), 1)
plotDf <- blogs_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g1 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity", fill="green") + coord_flip() +
    ggtitle("Blogs: 1-Gram Frequency Plot")

blogs_info <- text_processing(sample_lines(file_name, n=blogs_length*0.003, blogs_length), 2)
plotDf <- blogs_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g2 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity", fill="green") + coord_flip() +
    ggtitle("Blogs: 2-Gram Frequency Plot")

blogs_info <- text_processing(sample_lines(file_name, n=blogs_length*0.003, blogs_length), 3)
plotDf <- blogs_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g3 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity", fill="green") + coord_flip() +
    ggtitle("Blogs: 3-Gram Frequency Plot")
```

```{r echo=FALSE}
g1
```

```{r echo=FALSE}
g2
```


```{r echo=FALSE}
g3
```

## C (iii) News
```{r echo=FALSE}
file_name <- "en_US.news.txt"
news_info <- text_processing(sample_lines(file_name, n=news_length*0.003, news_length), 1)
plotDf <- news_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g1 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity",fill="yellow") + coord_flip() +
    ggtitle("News: 1-Gram Frequency Plot")

news_info <- text_processing(sample_lines(file_name, n=news_length*0.003, news_length), 2)
plotDf <- news_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g2 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity",fill="yellow") + coord_flip() +
    ggtitle("News: 2-Gram Frequency Plot")

news_info <- text_processing(sample_lines(file_name, n=news_length*0.003, news_length), 3)
plotDf <- news_info[1:20,]
plotDf$word <- reorder(plotDf$word, plotDf$frequency)
g3 = ggplot(plotDf, aes(x = word, y = frequency)) + geom_bar(stat = "identity",fill="yellow") + coord_flip() +
    ggtitle("News: 3-Gram Frequency Plot")

g1
```

```{r echo=FALSE}
g2
```


```{r echo=FALSE}
g3
```

## Conclusion
We used random sampling of 0.3% of the whole dataset due to memory limitations. Bootstrapping methodology will be used for the final product.

I have explained the major features of the data.

These insights will be channelled into creating the prediction algorithm and Shiny app

Documentation of the code can be found here
https://github.com/chen0350/JHUCapstone